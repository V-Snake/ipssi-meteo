# ğŸ—‚ï¸ IntÃ©gration HDFS avec Partitionnement par RÃ©gion

## Description

Cette implÃ©mentation intÃ¨gre HDFS dans l'architecture Kafka pour permettre le stockage persistant des donnÃ©es mÃ©tÃ©o avec partitionnement par rÃ©gion, continent et temps.

## Architecture HDFS

```
[Kafka Topics] â†’ [Spark HDFS Writer] â†’ [HDFS Partitioned Storage]
     â†“                    â†“                      â†“
weather_stream    spark-hdfs-writer    /weather-data/
                                           â”œâ”€â”€ continent=Europe/
                                           â”‚   â”œâ”€â”€ region=Europe/
                                           â”‚   â”‚   â”œâ”€â”€ year=2025/
                                           â”‚   â”‚   â”‚   â”œâ”€â”€ month=09/
                                           â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ day=22/
                                           â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ hour_partition=14/
                                           â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ *.parquet
                                           â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ hour_partition=15/
                                           â”‚   â”‚   â”‚   â”‚   â””â”€â”€ day=23/
                                           â”‚   â”‚   â”‚   â””â”€â”€ month=10/
                                           â”‚   â”‚   â””â”€â”€ year=2026/
                                           â”‚   â””â”€â”€ region=North America/
                                           â””â”€â”€ continent=Asia/
```

## Modifications ApportÃ©es

### 1. **Messages Enrichis** (kafka_producer_v2.py)

**Nouveau format des messages :**
```json
{
  "city": "London",
  "country": "United Kingdom",
  "admin1": "England",
  "region": "Europe",           // â† NOUVEAU: Pour partitionnement
  "continent": "Europe",        // â† NOUVEAU: Pour partitionnement
  "timestamp": 1758567241,
  "date": "2025-09-22",        // â† NOUVEAU: Pour partitionnement temporel
  "hour": "19",                // â† NOUVEAU: Pour partitionnement temporel
  "weather": {
    "temperature": 13.2,
    "windspeed": 11.8,
    "winddirection": 27,
    "weathercode": 0,
    "is_day": 0,
    "time": "2025-09-22T19:45"
  },
  "location_info": {
    "timezone": "Europe/London",
    "timezone_abbreviation": "GMT+1",
    "elevation": 23.0
  }
}
```

### 2. **Mapping RÃ©gion/Continent**

**Fonctions de mapping :**
- `get_region_from_country()` : DÃ©termine la rÃ©gion gÃ©ographique
- `get_continent_from_country()` : DÃ©termine le continent

**Exemples de mapping :**
- France â†’ Europe â†’ Europe
- United Kingdom â†’ Europe â†’ Europe  
- United States â†’ North America â†’ North America
- Japan â†’ Asia â†’ Asia

### 3. **Service Spark HDFS Writer**

**Fichier :** `spark-hdfs-writer/spark_hdfs_writer.py`

**FonctionnalitÃ©s :**
- Lit depuis `weather_stream`
- Partitionne par : `continent`, `region`, `year`, `month`, `day`, `hour_partition`
- Ã‰crit en format Parquet optimisÃ©
- Trigger toutes les 30 secondes

### 4. **SchÃ©mas Mis Ã  Jour**

**Tous les services Spark** ont Ã©tÃ© mis Ã  jour pour traiter :
- `region` : RÃ©gion gÃ©ographique
- `continent` : Continent
- `date` : Date au format YYYY-MM-DD
- `hour` : Heure au format HH

## Services Docker

### Nouveaux Services
```yaml
namenode:           # HDFS NameNode (Port 9870, 9000)
datanode:           # HDFS DataNode
spark-hdfs-writer:  # Service Spark pour Ã©crire dans HDFS
```

### Configuration HDFS
- **NameNode** : `hdfs://namenode:9000`
- **Web UI** : http://localhost:9870
- **Path** : `/weather-data`
- **Format** : Parquet (optimisÃ© pour analytics)

## Structure de Partitionnement

### Niveaux de Partitionnement
1. **continent** : Europe, North America, Asia, Africa, Oceania
2. **region** : Europe, North America, South America, Asia, Africa, Oceania
3. **year** : 2025, 2026, etc.
4. **month** : 01, 02, ..., 12
5. **day** : 01, 02, ..., 31
6. **hour_partition** : 00, 01, ..., 23

### Exemple de Structure
```
/weather-data/
â”œâ”€â”€ continent=Europe/
â”‚   â”œâ”€â”€ region=Europe/
â”‚   â”‚   â”œâ”€â”€ year=2025/
â”‚   â”‚   â”‚   â”œâ”€â”€ month=09/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ day=22/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ hour_partition=14/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ part-00000-xxx.parquet
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ part-00001-xxx.parquet
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ hour_partition=15/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ day=23/
â”‚   â”‚   â”‚   â””â”€â”€ month=10/
â”‚   â”‚   â””â”€â”€ year=2026/
â”‚   â””â”€â”€ region=North America/
â””â”€â”€ continent=Asia/
```

## Avantages du Partitionnement

### 1. **Performance de RequÃªte**
- Filtrage rapide par rÃ©gion/continent
- Scan minimal des donnÃ©es pertinentes
- Optimisation des requÃªtes analytiques

### 2. **Gestion des DonnÃ©es**
- Suppression facile des anciennes donnÃ©es
- Compression par partition
- Maintenance granulaire

### 3. **ScalabilitÃ©**
- Distribution des donnÃ©es
- ParallÃ©lisation des traitements
- Ã‰quilibrage de charge

## Test du SystÃ¨me

### Script de Test
```bash
./test_hdfs_partitioning.sh
```

### VÃ©rifications
1. **HDFS Status** : `hdfs dfsadmin -report`
2. **Structure** : `hdfs dfs -ls -R /weather-data`
3. **Web UI** : http://localhost:9870
4. **DonnÃ©es** : VÃ©rification des partitions crÃ©Ã©es

### Commandes Utiles
```bash
# Voir la structure HDFS
docker-compose exec namenode hdfs dfs -ls -R /weather-data

# Voir les donnÃ©es d'une partition
docker-compose exec namenode hdfs dfs -cat /weather-data/continent=Europe/region=Europe/year=2025/month=09/day=22/hour_partition=14/part-*.parquet

# VÃ©rifier l'espace disque
docker-compose exec namenode hdfs dfs -du -h /weather-data
```

## IntÃ©gration avec l'Architecture Existante

### Flux de DonnÃ©es Complet
```
[Producers] â†’ [Kafka] â†’ [Spark-APP] â†’ [Kafka] â†’ [Spark-AGGREGATES] â†’ [Kafka]
     â†“           â†“          â†“           â†“            â†“               â†“
[Producers] â†’ [Kafka] â†’ [Spark-HDFS-Writer] â†’ [HDFS Partitioned]
```

### Services ImpactÃ©s
- **kafka_producer_v2.py** : Messages enrichis
- **spark-aggregates.py** : SchÃ©mas mis Ã  jour
- **spark-hdfs-writer.py** : Nouveau service
- **docker-compose.yml** : Services HDFS ajoutÃ©s

Cette intÃ©gration HDFS permet un stockage persistant optimisÃ© pour les analyses gÃ©ographiques et temporelles ! ğŸ¯
