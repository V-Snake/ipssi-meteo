from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator


def check_spark_environment():
    """VÃ©rifier que l'environnement Spark est prÃªt"""
    print("ðŸ” Checking Spark environment...")
    print("âœ… Spark environment is ready")
    return "Spark OK"


def trigger_spark_transform():
    """DÃ©clencher le job Spark de transformation"""
    print("ðŸš€ Triggering Spark transformation job...")
    print("âœ… Spark transform job completed")
    return "Transform OK"


def trigger_spark_aggregates():
    """DÃ©clencher le job Spark d'agrÃ©gation"""
    print("ðŸ“Š Triggering Spark aggregates job...")
    print("âœ… Spark aggregates job completed")
    return "Aggregates OK"


def trigger_hdfs_writer():
    """DÃ©clencher l'Ã©criture HDFS"""
    print("ðŸ’¾ Triggering HDFS write job...")
    print("âœ… HDFS write job completed")
    return "HDFS Write OK"


default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=2),
}


with DAG(
    dag_id="spark_job_orchestration",
    description="Orchestrate Spark jobs for weather data processing",
    default_args=default_args,
    start_date=datetime(2025, 1, 1),
    schedule_interval=timedelta(hours=1),  # Toutes les heures
    catchup=False,
    max_active_runs=1,
) as dag:

    # 1. VÃ©rifier l'environnement Spark
    check_spark = PythonOperator(
        task_id="check_spark_environment",
        python_callable=check_spark_environment,
    )

    # 2. Lancer le job de transformation
    spark_transform = BashOperator(
        task_id="run_spark_transform",
        bash_command="""
        echo "ðŸš€ Starting Spark transformation job..."
        # Ici tu pourrais lancer: docker compose up -d spark-app
        echo "âœ… Spark transformation job started"
        """,
    )

    # 3. Attendre que la transformation soit terminÃ©e
    wait_for_transform = BashOperator(
        task_id="wait_for_transform_completion",
        bash_command="""
        echo "â³ Waiting for transformation to complete..."
        sleep 30  # Simuler l'attente
        echo "âœ… Transformation completed"
        """,
    )

    # 4. Lancer le job d'agrÃ©gation
    spark_aggregates = BashOperator(
        task_id="run_spark_aggregates",
        bash_command="""
        echo "ðŸ“Š Starting Spark aggregates job..."
        # Ici tu pourrais lancer: docker compose up -d spark-aggregates
        echo "âœ… Spark aggregates job started"
        """,
    )

    # 5. Lancer l'Ã©criture HDFS
    hdfs_writer = BashOperator(
        task_id="run_hdfs_writer",
        bash_command="""
        echo "ðŸ’¾ Starting HDFS write job..."
        # Ici tu pourrais lancer: docker compose up -d spark-hdfs-writer
        echo "âœ… HDFS write job started"
        """,
    )

    # 6. VÃ©rification finale
    final_check = BashOperator(
        task_id="final_pipeline_check",
        bash_command="""
        echo "ðŸ” Final pipeline check..."
        echo "âœ… All Spark jobs completed successfully"
        echo "ðŸ“Š Weather data pipeline is healthy"
        """,
    )

    # DÃ©finir les dÃ©pendances
    check_spark >> spark_transform >> wait_for_transform
    wait_for_transform >> [spark_aggregates, hdfs_writer]
    [spark_aggregates, hdfs_writer] >> final_check
