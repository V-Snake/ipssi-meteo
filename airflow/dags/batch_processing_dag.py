from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.providers.http.sensors.http import HttpSensor
import requests
import json

# Configuration par dÃ©faut
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# DAG de batch processing quotidien
with DAG(
    'batch_processing_daily',
    default_args=default_args,
    description='Batch processing quotidien pour la couche Lambda',
    schedule_interval='0 2 * * *',  # Tous les jours Ã  2h du matin
    catchup=False,
    tags=['batch', 'lambda', 'hdfs', 'analytics'],
) as dag:

    # 1. VÃ©rification de la santÃ© des services
    def check_hdfs_health():
        """VÃ©rifie la santÃ© de HDFS"""
        try:
            response = requests.get('http://namenode:9870/webhdfs/v1/?op=LISTSTATUS', timeout=10)
            if response.status_code == 200:
                print("âœ… HDFS NameNode est accessible")
                return True
            else:
                print(f"âŒ HDFS NameNode retourne le code {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ Erreur de connexion HDFS: {e}")
            return False

    def check_analytics_api_health():
        """VÃ©rifie la santÃ© de l'API Analytics"""
        try:
            response = requests.get('http://analytics-api:8000/health', timeout=10)
            if response.status_code == 200:
                print("âœ… Analytics API est accessible")
                return True
            else:
                print(f"âŒ Analytics API retourne le code {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ Erreur de connexion Analytics API: {e}")
            return False

    # 2. TÃ¢ches de vÃ©rification
    check_hdfs = PythonOperator(
        task_id='check_hdfs_health',
        python_callable=check_hdfs_health,
    )

    check_analytics = PythonOperator(
        task_id='check_analytics_health',
        python_callable=check_analytics_api_health,
    )

    # 3. TÃ¢ches de batch processing
    create_hdfs_directories = BashOperator(
        task_id='create_hdfs_directories',
        bash_command="""
        echo "CrÃ©ation des rÃ©pertoires HDFS pour le batch processing..."
        # CrÃ©er les rÃ©pertoires de partition par date
        DATE=$(date +%Y-%m-%d)
        echo "Date du batch: $DATE"
        """,
    )

    # 4. TÃ¢che de traitement des donnÃ©es historiques
    def process_historical_data():
        """Traite les donnÃ©es historiques pour le batch layer"""
        print("ðŸ”„ DÃ©but du traitement des donnÃ©es historiques...")
        
        # Simulation du traitement batch
        # Dans un vrai pipeline, on ferait :
        # 1. Lire les donnÃ©es de Kafka depuis la veille
        # 2. Les agrÃ©ger par ville/pays
        # 3. Calculer les statistiques (moyennes, max, min)
        # 4. Sauvegarder dans HDFS avec partitionnement
        
        print("âœ… Traitement des donnÃ©es historiques terminÃ©")
        return "Batch processing completed successfully"

    process_batch_data = PythonOperator(
        task_id='process_historical_data',
        python_callable=process_historical_data,
    )

    # 5. TÃ¢che de gÃ©nÃ©ration des rapports
    def generate_daily_report():
        """GÃ©nÃ¨re un rapport quotidien via l'API Analytics"""
        try:
            # Appel Ã  l'API pour gÃ©nÃ©rer un rapport
            response = requests.get('http://analytics-api:8000/reports/daily', timeout=30)
            if response.status_code == 200:
                report_data = response.json()
                print(f"âœ… Rapport quotidien gÃ©nÃ©rÃ©: {report_data}")
                return report_data
            else:
                print(f"âŒ Erreur gÃ©nÃ©ration rapport: {response.status_code}")
                return None
        except Exception as e:
            print(f"âŒ Erreur lors de la gÃ©nÃ©ration du rapport: {e}")
            return None

    generate_report = PythonOperator(
        task_id='generate_daily_report',
        python_callable=generate_daily_report,
    )

    # 6. TÃ¢che de nettoyage
    cleanup_temp_files = BashOperator(
        task_id='cleanup_temp_files',
        bash_command="""
        echo "ðŸ§¹ Nettoyage des fichiers temporaires..."
        # Nettoyer les fichiers temporaires du batch processing
        echo "âœ… Nettoyage terminÃ©"
        """,
    )

    # 7. TÃ¢che de notification
    def send_completion_notification():
        """Envoie une notification de fin de batch processing"""
        print("ðŸ“§ Envoi de notification de fin de batch processing...")
        # Ici on pourrait envoyer un email ou une notification Slack
        print("âœ… Notification envoyÃ©e")
        return "Notification sent"

    send_notification = PythonOperator(
        task_id='send_completion_notification',
        python_callable=send_completion_notification,
    )

    # DÃ©finition des dÃ©pendances
    [check_hdfs, check_analytics] >> create_hdfs_directories >> process_batch_data >> generate_report >> cleanup_temp_files >> send_notification
