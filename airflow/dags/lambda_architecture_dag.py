from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.sensors.filesystem import FileSensor
import requests
import json

# Configuration par dÃ©faut
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# DAG principal de l'architecture Lambda
with DAG(
    'lambda_architecture_pipeline',
    default_args=default_args,
    description='Pipeline complet de l\'architecture Lambda pour la mÃ©tÃ©o',
    schedule_interval='@hourly',  # Toutes les heures
    catchup=False,
    tags=['lambda', 'architecture', 'weather', 'pipeline'],
) as dag:

    # ===== PHASE 1: SPEED LAYER (Temps rÃ©el) =====
    speed_layer_start = DummyOperator(
        task_id='speed_layer_start',
    )

    def start_speed_layer():
        """DÃ©marre la couche Speed (temps rÃ©el)"""
        print("ğŸš€ DÃ©marrage de la Speed Layer...")
        print("- Kafka Producers: âœ… Actifs")
        print("- Spark Streaming: âœ… Actif") 
        print("- Real-time Consumers: âœ… Actifs")
        return "Speed Layer started"

    start_speed = PythonOperator(
        task_id='start_speed_layer',
        python_callable=start_speed_layer,
    )

    def monitor_speed_layer():
        """Surveille la couche Speed"""
        print("ğŸ“Š Surveillance de la Speed Layer...")
        # VÃ©rifier les mÃ©triques Kafka
        # VÃ©rifier les performances Spark Streaming
        print("âœ… Speed Layer opÃ©rationnelle")
        return "Speed Layer monitored"

    monitor_speed = PythonOperator(
        task_id='monitor_speed_layer',
        python_callable=monitor_speed_layer,
    )

    # ===== PHASE 2: BATCH LAYER (Traitement par lots) =====
    batch_layer_start = DummyOperator(
        task_id='batch_layer_start',
    )

    def start_batch_layer():
        """DÃ©marre la couche Batch"""
        print("ğŸ”„ DÃ©marrage de la Batch Layer...")
        print("- HDFS: âœ… Actif")
        print("- Spark Batch Jobs: âœ… Actifs")
        print("- Analytics API: âœ… Actif")
        return "Batch Layer started"

    start_batch = PythonOperator(
        task_id='start_batch_layer',
        python_callable=start_batch_layer,
    )

    def process_batch_data():
        """Traite les donnÃ©es en batch"""
        print("ğŸ“ˆ Traitement des donnÃ©es batch...")
        # Simulation du traitement batch
        print("- AgrÃ©gation des donnÃ©es par ville")
        print("- Calcul des statistiques historiques")
        print("- Sauvegarde dans HDFS avec partitionnement")
        print("âœ… Traitement batch terminÃ©")
        return "Batch processing completed"

    process_batch = PythonOperator(
        task_id='process_batch_data',
        python_callable=process_batch_data,
    )

    # ===== PHASE 3: SERVING LAYER (Service) =====
    serving_layer_start = DummyOperator(
        task_id='serving_layer_start',
    )

    def start_serving_layer():
        """DÃ©marre la couche Serving"""
        print("ğŸŒ DÃ©marrage de la Serving Layer...")
        print("- Grafana Dashboards: âœ… Actifs")
        print("- Analytics API: âœ… Actif")
        print("- ClickHouse: âœ… Actif")
        return "Serving Layer started"

    start_serving = PythonOperator(
        task_id='start_serving_layer',
        python_callable=start_serving_layer,
    )

    def generate_unified_view():
        """GÃ©nÃ¨re une vue unifiÃ©e des donnÃ©es"""
        print("ğŸ”— GÃ©nÃ©ration de la vue unifiÃ©e...")
        print("- Fusion des donnÃ©es temps rÃ©el et batch")
        print("- Mise Ã  jour des dashboards")
        print("- GÃ©nÃ©ration des rapports")
        print("âœ… Vue unifiÃ©e gÃ©nÃ©rÃ©e")
        return "Unified view generated"

    generate_unified = PythonOperator(
        task_id='generate_unified_view',
        python_callable=generate_unified_view,
    )

    # ===== PHASE 4: MONITORING ET VALIDATION =====
    def validate_lambda_architecture():
        """Valide le bon fonctionnement de l'architecture Lambda"""
        print("ğŸ” Validation de l'architecture Lambda...")
        
        # VÃ©rifications Speed Layer
        print("âœ… Speed Layer: DonnÃ©es temps rÃ©el traitÃ©es")
        
        # VÃ©rifications Batch Layer  
        print("âœ… Batch Layer: DonnÃ©es historiques traitÃ©es")
        
        # VÃ©rifications Serving Layer
        print("âœ… Serving Layer: APIs et dashboards opÃ©rationnels")
        
        print("ğŸ‰ Architecture Lambda validÃ©e avec succÃ¨s!")
        return "Lambda architecture validated"

    validate_architecture = PythonOperator(
        task_id='validate_lambda_architecture',
        python_callable=validate_lambda_architecture,
    )

    def send_lambda_status():
        """Envoie le statut de l'architecture Lambda"""
        print("ğŸ“Š Statut de l'architecture Lambda:")
        print("ğŸŸ¢ Speed Layer: OpÃ©rationnelle")
        print("ğŸŸ¢ Batch Layer: OpÃ©rationnelle") 
        print("ğŸŸ¢ Serving Layer: OpÃ©rationnelle")
        print("ğŸ¯ Architecture Lambda complÃ¨tement fonctionnelle!")
        return "Lambda status sent"

    send_status = PythonOperator(
        task_id='send_lambda_status',
        python_callable=send_lambda_status,
    )

    # ===== DÃ‰FINITION DES DÃ‰PENDANCES =====
    
    # Phase 1: Speed Layer (parallÃ¨le)
    speed_layer_start >> start_speed >> monitor_speed
    
    # Phase 2: Batch Layer (parallÃ¨le avec Speed)
    batch_layer_start >> start_batch >> process_batch
    
    # Phase 3: Serving Layer (aprÃ¨s Speed et Batch)
    [monitor_speed, process_batch] >> serving_layer_start >> start_serving >> generate_unified
    
    # Phase 4: Validation finale
    generate_unified >> validate_architecture >> send_status
